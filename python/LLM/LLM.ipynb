{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Handle Text\n",
    "\n",
    "## Text Tokenization\n",
    "\n",
    "### Download and Read the Verdict\n",
    "Download the Verdict as a text file, then read the file content."
   ],
   "id": "463d076a7403d0fb"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-01T09:46:49.996164Z",
     "start_time": "2025-11-01T09:46:49.980127Z"
    }
   },
   "source": [
    "import urllib.request\n",
    "from ftplib import ftpcp\n",
    "from msilib import type_key\n",
    "\n",
    "# url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "#        \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "#        \"the-verdict.txt\")\n",
    "# file_path = \"./the-verdict.txt\"\n",
    "# urllib.request.urlretrieve(url, file_path)\n",
    "\n",
    "file_path = \"./the-verdict.txt\"\n",
    "with open(file_path, \"r\") as file:\n",
    "    raw_text = file.read()\n",
    "print(f\"Total characters: {len(raw_text)}\")\n",
    "print(raw_text[:100])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no g\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Split the Text into Words Using Regex",
   "id": "1c4c96efd9d908e8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T09:46:50.106340Z",
     "start_time": "2025-11-01T09:46:50.083949Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(len(preprocessed))"
   ],
   "id": "6865b8430a1474d7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Convert the Words to Unique IDs\n",
   "id": "b75a48137d4d231e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T09:46:50.199337Z",
     "start_time": "2025-11-01T09:46:50.190626Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "print(vocab_size)\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break"
   ],
   "id": "dabdd5fc4bdd2e03",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n",
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Simple Tokenizer\n",
    "Implement a encode function to convert a text into a sequence of token IDs.\n",
    "\n",
    "Implement a decode function to convert a sequence of token IDs back into a text."
   ],
   "id": "23f4347a09f1b8e3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T09:46:50.306781Z",
     "start_time": "2025-11-01T09:46:50.294080Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.inverse_vocab = {integer:token for token, integer in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        return [self.vocab[token] for token in preprocessed]\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = ' '.join([self.inverse_vocab[id] for id in ids])\n",
    "        text = re.sub(r'\\s+([,.:;?_!\"()\\'])', r'\\1', text)\n",
    "        return text\n",
    "\n",
    "tokenizerV1 = SimpleTokenizerV1(vocab)\n",
    "text = \"\"\"It's the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\"\"\"\"\"\n",
    "ids = tokenizerV1.encode(text)\n",
    "print(ids)\n",
    "decoded_text = tokenizerV1.decode(ids)\n",
    "print(decoded_text)"
   ],
   "id": "1cae35ccaf02aa2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n",
      "It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Tokenizer V2\n",
    "We want to handle the unknown words in the text.\n",
    "We will add a special token `<|unk|>` to represent the unknown words.\n",
    "\n",
    "And add a token `<|endoftext|>` to represent the end of the text, thus we can handle multiple texts that from different sources."
   ],
   "id": "38f678f921606de"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T09:46:50.430292Z",
     "start_time": "2025-11-01T09:46:50.418335Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# add the special tokens to the vocab\n",
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
    "\n",
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.inverse_vocab = {integer:token for token, integer in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        return [self.vocab.get(token, self.vocab[\"<|unk|>\"]) for token in preprocessed]\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = ' '.join([self.inverse_vocab[id] for id in ids])\n",
    "        text = re.sub(r'\\s+([,.:;?_!\"()\\'])', r'\\1', text)\n",
    "        return text\n",
    "\n",
    "tokenizerV2 = SimpleTokenizerV2(vocab)\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "ids = tokenizerV2.encode(text)\n",
    "print(ids)\n",
    "decoded_text = tokenizerV2.decode(ids)\n",
    "print(decoded_text)\n"
   ],
   "id": "54496b82e79e7e22",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]\n",
      "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### BPE Tokenizer\n",
    "BPE could handle the unknown words in the text by split it into smaller tokens.\n",
    "\n",
    "BPE merge the most frequent pairs of tokens into a new token to make the vocabulary."
   ],
   "id": "55d467d2cf68a428"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T09:46:50.540249Z",
     "start_time": "2025-11-01T09:46:50.528812Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "text = \"Aiwerkn oker\"\n",
    "ids = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(ids)\n",
    "for id in ids:\n",
    "    print(id, tokenizer.decode([id]))\n",
    "print(tokenizer.decode(ids))"
   ],
   "id": "cebd7becb1ac5db9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32, 14246, 9587, 77, 267, 6122]\n",
      "32 A\n",
      "14246 iw\n",
      "9587 erk\n",
      "77 n\n",
      "267  o\n",
      "6122 ker\n",
      "Aiwerkn oker\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Use Sliding Window to Create Input-Output Pairs\n",
    "### Read and Encode the Verdict"
   ],
   "id": "75b4d72d4d495fd7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T09:46:50.633278Z",
     "start_time": "2025-11-01T09:46:50.618102Z"
    }
   },
   "cell_type": "code",
   "source": [
    "file_path = \"./the-verdict.txt\"\n",
    "with open(file_path, \"r\") as file:\n",
    "    raw_text = file.read()\n",
    "enc_text = tokenizer.encode(raw_text, allowed_special={\"<|endoftext|>\"})\n",
    "print(len(enc_text))"
   ],
   "id": "c95e6e986b98f227",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Create Input-Output Pairs\n",
    "The input will be a sequence of tokens.\n",
    "\n",
    "The output will be the next token in the sequence."
   ],
   "id": "63b6c1a39352d381"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T09:46:50.728477Z",
     "start_time": "2025-11-01T09:46:50.714860Z"
    }
   },
   "cell_type": "code",
   "source": [
    "enc_sample = enc_text[:10]\n",
    "context_size = 4\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size + 1]\n",
    "\n",
    "for i in range(1, context_size + 1):\n",
    "    context = enc_sample[:i]\n",
    "    target = enc_sample[i]\n",
    "    print(context, \"->\", target)\n",
    "    print(tokenizer.decode(context), \"->\", tokenizer.decode([target]))"
   ],
   "id": "316d095a9cb58efa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40] -> 367\n",
      "I ->  H\n",
      "[40, 367] -> 2885\n",
      "I H -> AD\n",
      "[40, 367, 2885] -> 1464\n",
      "I HAD ->  always\n",
      "[40, 367, 2885, 1464] -> 1807\n",
      "I HAD always ->  thought\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### DataSet and DataLoader\n",
    "We use the sliding window to create the input-output pairs.\n",
    "\n",
    "x is the input sequence of tokens, specifically `text[sample_start: sample_start + context_size]`.\n",
    "\n",
    "y is the target sequence of tokens, specifically `text[sample_start + 1: sample_start + context_size + 1]`.\n",
    "\n",
    "We could generate the input-output pairs from x and y as forementioned.\n",
    "\n",
    "DataSet is to store the tokenized text. DataLoader is to load the data in batches."
   ],
   "id": "2db3f57302f23929"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T09:46:50.869571Z",
     "start_time": "2025-11-01T09:46:50.818449Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDataSetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size = 4, max_length = 256, stride = 128, shuffle = True, drop_last = True, num_workers = 0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDataSetV1(txt, tokenizer, max_length, stride)\n",
    "    return DataLoader(dataset, batch_size = batch_size, shuffle = shuffle, drop_last = drop_last, num_workers = num_workers)\n",
    "\n",
    "file_path = \"./the-verdict.txt\"\n",
    "with open(file_path, \"r\") as file:\n",
    "    raw_text = file.read()\n",
    "\n",
    "dataLoader = create_dataloader_v1(raw_text, batch_size = 8, max_length = 4, stride = 4, shuffle = False, drop_last = True, num_workers = 0)\n",
    "data_iter = iter(dataLoader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)\n"
   ],
   "id": "5870736813c53745",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]]), tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])]\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Embedding Layer\n",
    "\n",
    "### A Simple Embedding Layer\n",
    "As a simple example, we create an embedding layer.\n",
    "\n",
    "The embedding layer has two parameters:\n",
    "1. The vocabulary size, which is the number of unique tokens in the dataset.\n",
    "2. The embedding dimension, which is the size of the vector representation for each token.\n",
    "\n",
    "The embedding layer receives a tensor of token indices and returns a tensor of token embeddings.\n",
    "For example, the output dim is 5 and the num of tokens is 32, then it will return a tensor of shape (5, 32)."
   ],
   "id": "7e7c000ba4ab83f6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T09:46:51.058474Z",
     "start_time": "2025-11-01T09:46:50.927743Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vocab_size = tokenizer.n_vocab\n",
    "output_dim = 256\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "\n",
    "print(embedding_layer(torch.tensor([3])))"
   ],
   "id": "19e756358679a889",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.1796e+00,  2.6596e+00, -1.3225e+00, -1.4284e-01, -1.8063e+00,\n",
      "          7.2274e-01, -7.7692e-01, -1.3758e+00,  4.2831e-02, -2.3736e-03,\n",
      "         -3.9243e-01,  1.7003e+00, -1.1849e+00,  1.4882e+00,  3.3147e+00,\n",
      "         -7.8773e-01, -8.3949e-01,  3.3218e-01, -2.2851e-01,  2.6071e+00,\n",
      "          1.1681e+00, -9.7290e-01,  1.0955e+00, -5.6462e-01, -3.6621e-01,\n",
      "          3.9884e-01, -1.1182e+00, -3.6302e-01, -1.2505e-01,  8.3438e-01,\n",
      "          3.9030e-01, -3.3310e-01, -1.5212e+00, -1.0996e+00, -1.3988e-01,\n",
      "          1.0753e-01, -1.8687e-01, -1.3451e+00, -2.6970e+00, -2.5137e-01,\n",
      "          1.3419e+00, -7.6217e-01,  1.5102e-01, -2.8172e+00, -1.7896e-01,\n",
      "          2.3313e-01,  2.5726e-01,  6.9367e-01,  8.3333e-01, -5.1383e-01,\n",
      "          2.9276e+00, -5.9818e-01,  4.2266e-02,  1.5295e+00, -4.3006e-01,\n",
      "          9.3392e-01,  6.5517e-01,  2.2213e-01,  1.6212e+00,  9.0254e-01,\n",
      "          1.4859e+00,  3.5938e-02, -1.8313e+00, -5.1683e-02, -6.0639e-01,\n",
      "         -2.4435e-01,  1.8822e+00, -1.9907e+00, -6.7811e-01,  1.7133e+00,\n",
      "          5.2290e-01,  1.5533e-01, -1.0323e+00,  2.1163e+00, -9.9698e-02,\n",
      "         -9.4159e-01,  1.0849e+00, -7.3333e-01, -6.9474e-02, -1.6288e-01,\n",
      "         -1.3748e+00, -3.9968e-02,  1.5447e+00, -1.6788e-01, -2.9722e-01,\n",
      "          1.3107e-01, -3.9299e+00,  3.4351e-01, -1.1863e+00,  9.3869e-01,\n",
      "          7.8073e-01,  1.4366e+00,  8.1069e-01, -5.4606e-01,  6.3635e-01,\n",
      "         -6.9496e-02,  8.6487e-01,  7.7396e-01, -6.5551e-01,  1.0661e+00,\n",
      "         -1.6025e+00, -1.4528e+00, -3.7307e-01,  2.2988e-01, -1.0407e+00,\n",
      "          8.9035e-01, -1.7226e+00,  5.3059e-02, -5.3608e-01, -1.7982e-01,\n",
      "         -1.0932e+00,  5.1557e-01, -1.2824e-01, -4.0030e-01,  5.2146e-01,\n",
      "          2.3166e-01,  7.1488e-01, -1.0154e+00, -1.0612e+00,  1.3935e+00,\n",
      "          4.1097e-01,  8.3034e-01,  5.7284e-01, -1.4132e-01, -1.8596e+00,\n",
      "          7.6133e-01,  1.2289e+00, -8.7367e-01,  1.8150e-01, -6.6697e-01,\n",
      "         -1.8066e-02, -6.8900e-01,  4.5434e-01, -1.5157e+00,  4.1730e-01,\n",
      "          1.0526e+00,  9.3685e-01,  5.7820e-01, -1.4329e+00, -5.2512e-01,\n",
      "         -1.0893e+00,  1.3882e+00, -3.0871e-01,  1.4557e+00,  2.5464e-01,\n",
      "          5.9587e-01,  3.2534e-01,  8.2288e-01,  6.8585e-02,  1.6707e-02,\n",
      "          1.0355e+00,  3.2153e-01, -1.4255e+00, -4.7012e-01,  7.7757e-01,\n",
      "         -1.7148e+00,  1.3970e-01,  1.6818e+00, -1.0315e+00, -8.7508e-01,\n",
      "         -1.4407e+00, -2.8184e-01, -1.4996e+00, -1.2717e+00,  1.1513e+00,\n",
      "         -1.2240e+00,  5.3442e-01, -2.2221e+00, -7.0441e-01,  9.3900e-02,\n",
      "         -7.2561e-01,  8.3950e-01, -8.7233e-01,  1.1525e+00,  1.5587e+00,\n",
      "         -1.8793e-01,  8.0241e-01,  1.9697e+00,  1.9045e+00,  9.2840e-02,\n",
      "         -1.6816e+00, -5.1690e-01,  8.3930e-01, -8.2973e-02, -2.8395e-01,\n",
      "          1.1069e+00,  1.8132e+00, -2.3730e-01, -2.0497e-01,  3.6592e-01,\n",
      "         -1.2418e+00,  3.9737e-01,  3.7013e-02,  4.2207e-01,  2.7477e-01,\n",
      "         -2.6073e-01,  3.4295e-01,  7.1739e-01, -1.0693e+00,  4.6599e-01,\n",
      "         -7.8713e-01, -8.8243e-01,  1.8951e+00,  3.3650e-01, -7.8066e-01,\n",
      "          1.1682e-01, -2.0416e+00, -2.7491e-01,  7.1034e-02, -7.0391e-01,\n",
      "         -1.4885e+00,  8.0801e-01,  1.6744e+00,  1.4403e-02,  1.2517e+00,\n",
      "         -1.2915e+00,  6.2912e-01,  1.0272e+00,  2.6078e-01, -3.2296e-01,\n",
      "          1.6613e+00, -9.9332e-01, -7.0100e-01, -9.4900e-01,  2.4119e-01,\n",
      "          1.7627e+00, -3.5023e-01,  1.2668e+00,  6.1410e-02,  5.3659e-01,\n",
      "          2.3255e-01, -2.9999e-01,  6.1961e-01, -3.4853e-01, -1.9876e-01,\n",
      "          6.9974e-01,  9.3670e-01, -8.9075e-01, -4.3232e-01,  1.3087e+00,\n",
      "          9.1792e-01,  8.3201e-01, -1.1345e-02,  1.6159e+00, -5.6418e-01,\n",
      "         -9.4077e-01,  5.6002e-01,  1.7931e+00,  1.1205e+00, -1.3817e+00,\n",
      "         -3.2333e-01, -2.3261e-01,  2.3828e-01,  1.6890e-01, -1.1292e+00,\n",
      "          4.5102e-01]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Positional Embedding\n",
    "Fixed embedding cannot capture the position of tokens in a sequence.\n",
    "\n",
    "Since the position of tokens does matter with the meaning of the sequence, we could use positional embedding to add the position information to the token embeddings.\n",
    "\n",
    "There are two positional embedding methods:\n",
    "1. Absolute positional embedding\n",
    "2. Relative positional embedding"
   ],
   "id": "7e69b68eba85d211"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T09:46:51.152948Z",
     "start_time": "2025-11-01T09:46:51.114292Z"
    }
   },
   "cell_type": "code",
   "source": [
    "max_length = 4\n",
    "dataLoader = create_dataloader_v1(raw_text, batch_size = 8, max_length = max_length, stride = max_length, shuffle = False, drop_last = True, num_workers = 0)\n",
    "data_iter = iter(dataLoader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Input shape:\", inputs.shape)\n",
    "\n",
    "token_embeddings = embedding_layer(inputs)\n",
    "print(\"Input token embeddings shape:\", token_embeddings.shape)\n",
    "\n",
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "print(\"Positional embeddings shape:\", pos_embeddings.shape)\n",
    "\n",
    "# add the positional embeddings to the token embeddings\n",
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(\"Input embeddings shape with positional embeddings:\", input_embeddings.shape)\n"
   ],
   "id": "c37e74da05bba794",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([8, 4])\n",
      "Input token embeddings shape: torch.Size([8, 4, 256])\n",
      "Positional embeddings shape: torch.Size([4, 256])\n",
      "Input embeddings shape with positional embeddings: torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Attention Mechanism\n",
    "The attention mechanism means for each token in sequence, it will add a weighted sum of all tokens in the sequence, thus it could .\n",
    "\n",
    "We will compute the attention score for each token in the sequence.\n",
    "\n",
    "## Simple Attention Mechanism Without Trainable Parameters"
   ],
   "id": "f13dd5f4ebb4ff5e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T09:46:51.262766Z",
     "start_time": "2025-11-01T09:46:51.229399Z"
    }
   },
   "cell_type": "code",
   "source": [
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89],\n",
    "    [0.55, 0.82, 0.63],\n",
    "    [0.22, 0.18, 0.05],\n",
    "    [0.76, 0.59, 0.92]]\n",
    ")\n",
    "\n",
    "# we assume that the attention score is the dot product of the query and the key\n",
    "# we will query the first token\n",
    "# attention score for the first token is 0.43*0.43 + 0.15*0.15 + 0.89*0.89\n",
    "# attention score for the second token is 0.55*0.43 + 0.82*0.15 + 0.63*0.89\n",
    "# attention score for the third token is 0.22*0.43 + 0.18*0.15 + 0.05*0.89\n",
    "# attention score for the fourth token is 0.76*0.43 + 0.59*0.15 + 0.92*0.89\n",
    "query = inputs[0]\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(query, x_i)\n",
    "\n",
    "print(\"query.shape:\", query.shape)\n",
    "print(\"attn_scores_2.shape:\", attn_scores_2.shape)\n",
    "print(attn_scores_2)\n",
    "\n",
    "\n",
    "# then we normalize the attention scores to make them sum up to 1\n",
    "def softmax_naive(x):\n",
    "    exp_x = torch.exp(x)\n",
    "    return exp_x / torch.sum(exp_x)\n",
    "\n",
    "attn_weights_naive = softmax_naive(attn_scores_2)\n",
    "print(\"attn_weights_naive:\", attn_weights_naive)\n",
    "print(\"sum of attn_weights_naive:\", torch.sum(attn_weights_naive))\n",
    "\n",
    "attn_weights = torch.softmax(attn_scores_2, dim = 0)\n",
    "print(\"attn_weights:\", attn_weights)\n",
    "print(\"sum of attn_weights:\", torch.sum(attn_weights))\n",
    "\n",
    "# after we get the attention weights, we can compute the weighted sum of the values\n",
    "# the weighted sum of the values is the output of the attention mechanism\n",
    "context_vec_first = torch.zeros_like(query)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    context_vec_first += attn_weights[i] * x_i\n",
    "\n",
    "print(\"context_vec of the first token:\", context_vec_first)\n",
    "\n",
    "# compute all context vectors\n",
    "attn_scores = torch.empty(inputs.shape[0], inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "print(\"attn_scores:\", attn_scores)\n",
    "\n",
    "attn_scores = inputs @ inputs.T\n",
    "print(\"attn_scores:\", attn_scores)\n",
    "\n",
    "# then we normalize the attention scores, dim -1 means normalize along the last dimension\n",
    "attn_weights = torch.softmax(attn_scores, dim = -1)\n",
    "print(\"attn_weights:\", attn_weights)\n",
    "print(\"sum of attn_weights:\", torch.sum(attn_weights, dim = -1))\n"
   ],
   "id": "da19eff0291cfadb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query.shape: torch.Size([3])\n",
      "attn_scores_2.shape: torch.Size([4])\n",
      "tensor([0.9995, 0.9202, 0.1661, 1.2341])\n",
      "attn_weights_naive: tensor([0.2760, 0.2550, 0.1200, 0.3490])\n",
      "sum of attn_weights_naive: tensor(1.)\n",
      "attn_weights: tensor([0.2760, 0.2550, 0.1200, 0.3490])\n",
      "sum of attn_weights: tensor(1.0000)\n",
      "context_vec of the first token: tensor([0.5506, 0.4780, 0.7334])\n",
      "attn_scores: tensor([[0.9995, 0.9202, 0.1661, 1.2341],\n",
      "        [0.9202, 1.3718, 0.3001, 1.4814],\n",
      "        [0.1661, 0.3001, 0.0833, 0.3194],\n",
      "        [1.2341, 1.4814, 0.3194, 1.7721]])\n",
      "attn_scores: tensor([[0.9995, 0.9202, 0.1661, 1.2341],\n",
      "        [0.9202, 1.3718, 0.3001, 1.4814],\n",
      "        [0.1661, 0.3001, 0.0833, 0.3194],\n",
      "        [1.2341, 1.4814, 0.3194, 1.7721]])\n",
      "attn_weights: tensor([[0.2760, 0.2550, 0.1200, 0.3490],\n",
      "        [0.2057, 0.3231, 0.1106, 0.3605],\n",
      "        [0.2364, 0.2703, 0.2176, 0.2756],\n",
      "        [0.2276, 0.2914, 0.0912, 0.3898]])\n",
      "sum of attn_weights: tensor([1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Oringinal Transformer\n",
    "There are three trainable matrix:\n",
    "1. Query matrix\n",
    "2. Key matrix\n",
    "3. Value matrix\n",
    "\n",
    "We will use the query matrix to compute the attention score for each token in the sequence.\n",
    "\n",
    "### Calculation of Transformer\n",
    "\n",
    "queries = inputs @ W_query\n",
    "\n",
    "keys = inputs @ W_key\n",
    "\n",
    "values = inputs @ W_value\n",
    "\n",
    "\n",
    "\n",
    "The attention score from token i to token j equals queries[i] dot with keys[j].\n",
    "\n",
    "We use softmax to nomalize the attention score to attention weight.\n",
    "\n",
    "Then we use the weight[i, j] multiple with values[j] to get the contex vrctor[i, j]."
   ],
   "id": "622aba7286604517"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T09:46:51.370052Z",
     "start_time": "2025-11-01T09:46:51.339436Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_dim = 3\n",
    "output_dim = 2\n",
    "\n",
    "torch.manual_seed(123)\n",
    "W_query = torch.randn(input_dim, output_dim)\n",
    "W_key = torch.randn(input_dim, output_dim)\n",
    "W_value = torch.randn(input_dim, output_dim)\n",
    "\n",
    "queries = inputs @ W_query\n",
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "print(\"queries:\", queries)\n",
    "print(\"keys:\", keys)\n",
    "print(\"values:\", values)\n",
    "\n",
    "print(\"queries.shape:\", queries.shape)\n",
    "print(\"keys.shape\", keys.shape)\n",
    "print(\"values.shape\", values.shape)\n",
    "\n",
    "query_2 = queries[1]\n",
    "keys_2 = keys[1]\n",
    "attn_scores_2 = query_2 @ keys.T\n",
    "\n",
    "dim_k = keys.shape[-1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / dim_k**0.5, dim = -1)\n",
    "print(\"attn_weights_2:\", attn_weights_2)\n",
    "\n",
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(\"context_vec_2\", context_vec_2)\n"
   ],
   "id": "10df03eba8a36a66",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queries: tensor([[-1.1686e+00,  2.0194e-01],\n",
      "        [-1.1185e+00,  8.9658e-04],\n",
      "        [-1.5090e-01, -6.3319e-03],\n",
      "        [-1.4040e+00,  1.4216e-01]])\n",
      "keys: tensor([[-0.1823, -0.6888],\n",
      "        [-0.1367, -0.7505],\n",
      "        [-0.1451, -0.2052],\n",
      "        [-0.3544, -0.9974]])\n",
      "values: tensor([[ 0.1196, -0.3566],\n",
      "        [ 0.3942,  0.6054],\n",
      "        [ 0.1133,  0.2772],\n",
      "        [ 0.3512,  0.2643]])\n",
      "queries.shape: torch.Size([4, 2])\n",
      "keys.shape torch.Size([4, 2])\n",
      "values.shape torch.Size([4, 2])\n",
      "attn_weights_2: tensor([0.2450, 0.2363, 0.2380, 0.2807])\n",
      "context_vec_2 tensor([0.2480, 0.1958])\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Then we extend the calculation to the whole token serial.",
   "id": "ed218b417ba627de"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T09:46:51.464639Z",
     "start_time": "2025-11-01T09:46:51.445639Z"
    }
   },
   "cell_type": "code",
   "source": [
    "attn_scores = queries @ keys.T\n",
    "print(\"attn_scores:\", attn_scores)\n",
    "attn_weights = torch.softmax(attn_scores, dim = -1)\n",
    "print(\"attn_weights:\", attn_weights)"
   ],
   "id": "62650b92b9ff6d69",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_scores: tensor([[0.0740, 0.0082, 0.1281, 0.2127],\n",
      "        [0.2033, 0.1522, 0.1621, 0.3955],\n",
      "        [0.0319, 0.0254, 0.0232, 0.0598],\n",
      "        [0.1581, 0.0852, 0.1745, 0.3557]])\n",
      "attn_weights: tensor([[0.2415, 0.2261, 0.2549, 0.2774],\n",
      "        [0.2426, 0.2305, 0.2328, 0.2940],\n",
      "        [0.2492, 0.2476, 0.2470, 0.2562],\n",
      "        [0.2401, 0.2232, 0.2441, 0.2926]])\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### A Simple Self Attention Class",
   "id": "36527754ab7c55d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T09:46:51.559016Z",
     "start_time": "2025-11-01T09:46:51.533020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.randn(d_in, d_out))\n",
    "        self.W_key = nn.Parameter(torch.randn(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.randn(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim = -1)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "# use linear layer\n",
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim = -1)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "    def __str__(self):\n",
    "        name = \"SelfAttention_v2\"\n",
    "        wquery = W_query.__str__()\n",
    "        wkey = W_key.__str__()\n",
    "        wvalue = W_value.__str__()\n",
    "        return name + \"\\n W_query:\" + wquery + \"\\n W_key:\" + wkey + \"\\n W_value:\" + wvalue + \"\\n\""
   ],
   "id": "8c977e1c9ec8e7fd",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T09:46:51.605601Z",
     "start_time": "2025-11-01T09:46:51.585026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(input_dim, output_dim)\n",
    "print(sa_v2(inputs))\n",
    "\n",
    "print(sa_v2.__str__())"
   ],
   "id": "e638c40b30e127e7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0621,  0.0899],\n",
      "        [-0.0636,  0.0900],\n",
      "        [-0.0608,  0.0901],\n",
      "        [-0.0637,  0.0899]], grad_fn=<MmBackward0>)\n",
      "SelfAttention_v2\n",
      " W_query:tensor([[-0.1115,  0.1204],\n",
      "        [-0.3696, -0.2404],\n",
      "        [-1.1969,  0.2093]])\n",
      " W_key:tensor([[-0.9724, -0.7550],\n",
      "        [ 0.3239, -0.1085],\n",
      "        [ 0.2103, -0.3908]])\n",
      " W_value:tensor([[ 0.2350,  0.6653],\n",
      "        [ 0.3528,  0.9728],\n",
      "        [-0.0386, -0.8861]])\n",
      "\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Practice to Transfer V2 Parameter to V1\n",
    "nn.Linear is the transpose of tensor, it's Tensor(output_dim, input_dim)\n",
    "\n",
    "This is because in the matrix multiply, the matrix 2 is visited by col and col, transpose it will make the memory visit continuously, thus increase the usage of hardware cache."
   ],
   "id": "e47548d9cd49f548"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T09:46:51.684799Z",
     "start_time": "2025-11-01T09:46:51.665789Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sa_v1 = SelfAttention_v1(input_dim, output_dim)\n",
    "sa_v1.W_value = nn.Parameter(sa_v2.W_value.weight.T)\n",
    "sa_v1.W_query = nn.Parameter(sa_v2.W_query.weight.T)\n",
    "sa_v1.W_key = nn.Parameter(sa_v2.W_key.weight.T)\n",
    "\n",
    "print(\"sa_v1 output:\", sa_v1(inputs))\n",
    "print(\"sa_v2 output:\", sa_v2(inputs))"
   ],
   "id": "61e99634e14e9ed7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sa_v1 output: tensor([[-0.0621,  0.0899],\n",
      "        [-0.0636,  0.0900],\n",
      "        [-0.0608,  0.0901],\n",
      "        [-0.0637,  0.0899]], grad_fn=<MmBackward0>)\n",
      "sa_v2 output: tensor([[-0.0621,  0.0899],\n",
      "        [-0.0636,  0.0900],\n",
      "        [-0.0608,  0.0901],\n",
      "        [-0.0637,  0.0899]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Causal Attention\n",
    "We use the mask to make sure that each token will only pay attention to tokens that before this token.\n",
    "\n",
    "### Mask"
   ],
   "id": "b8213fe18a4568c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T09:46:51.777610Z",
     "start_time": "2025-11-01T09:46:51.754611Z"
    }
   },
   "cell_type": "code",
   "source": [
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs)\n",
    "attn_scores = queries @ keys.T\n",
    "raw_attn_weights = torch.softmax(attn_scores / queries.shape[-1]**0.5, dim = -1)\n",
    "print(\"attn_weights:\", attn_weights)\n",
    "\n",
    "context_length = attn_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "print(\"mask_simple:\", mask_simple)\n",
    "\n",
    "masked_simple = mask_simple * attn_weights\n",
    "print(\"masked_simple:\", masked_simple)\n",
    "\n",
    "row_sums = masked_simple.sum(dim = -1, keepdim = True)\n",
    "masked_simple_norm = masked_simple / row_sums\n",
    "print(\"masked_simple_norm:\", masked_simple_norm)"
   ],
   "id": "97ad4a13c25e68d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_weights: tensor([[0.2415, 0.2261, 0.2549, 0.2774],\n",
      "        [0.2426, 0.2305, 0.2328, 0.2940],\n",
      "        [0.2492, 0.2476, 0.2470, 0.2562],\n",
      "        [0.2401, 0.2232, 0.2441, 0.2926]])\n",
      "mask_simple: tensor([[1., 0., 0., 0.],\n",
      "        [1., 1., 0., 0.],\n",
      "        [1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1.]])\n",
      "masked_simple: tensor([[0.2415, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2426, 0.2305, 0.0000, 0.0000],\n",
      "        [0.2492, 0.2476, 0.2470, 0.0000],\n",
      "        [0.2401, 0.2232, 0.2441, 0.2926]])\n",
      "masked_simple_norm: tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5128, 0.4872, 0.0000, 0.0000],\n",
      "        [0.3350, 0.3329, 0.3321, 0.0000],\n",
      "        [0.2401, 0.2232, 0.2441, 0.2926]])\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Another way to nomalize: make the mask to -inf, then it will get 0 after softmax.",
   "id": "9ea4b914f86a6a5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T09:46:51.855260Z",
     "start_time": "2025-11-01T09:46:51.837669Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "print(\"mask:\", mask)\n",
    "\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(\"masked:\", masked)\n",
    "\n",
    "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim = -1)\n",
    "print(\"attn_weights:\", attn_weights)"
   ],
   "id": "bd841907605ba189",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask: tensor([[0., 1., 1., 1.],\n",
      "        [0., 0., 1., 1.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0.]])\n",
      "masked: tensor([[0.2899,   -inf,   -inf,   -inf],\n",
      "        [0.4457, 0.1668,   -inf,   -inf],\n",
      "        [0.0970, 0.0404, 0.0055,   -inf],\n",
      "        [0.4739, 0.1588, 0.0266, 0.4337]], grad_fn=<MaskedFillBackward0>)\n",
      "attn_weights: tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5491, 0.4509, 0.0000, 0.0000],\n",
      "        [0.3450, 0.3315, 0.3234, 0.0000],\n",
      "        [0.2856, 0.2286, 0.2082, 0.2776]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Dropout\n",
    "Dropout is a way to mask that make some of the scores to zero randomly.\n",
    "\n",
    "Other numbers will multiply by (1 / possibility of dropout)."
   ],
   "id": "a08881ec1bae14e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T09:46:51.964160Z",
     "start_time": "2025-11-01T09:46:51.942193Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5)\n",
    "example = torch.ones(6, 6)\n",
    "dropped = dropout(example)\n",
    "print(dropped)\n",
    "\n",
    "print(dropout(attn_weights))"
   ],
   "id": "88c346840e1be34f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n",
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.6901, 0.0000, 0.6469, 0.0000],\n",
      "        [0.0000, 0.4571, 0.4164, 0.5552]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Causal Attention Implement",
   "id": "ff41f8d6c4fd3e67"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T09:46:52.057156Z",
     "start_time": "2025-11-01T09:46:52.035396Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        quries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = quries @ keys.transpose(-2, -1)\n",
    "        attn_scores.masked_fill(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim = -1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "batch = torch.stack((inputs, inputs, inputs), dim = 0)\n",
    "print(\"batch:\", batch)\n",
    "print(\"batch.shape:\", batch.shape)\n",
    "\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(input_dim, output_dim, context_length, 0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)\n",
    "print(\"context_vecs:\", context_vecs)"
   ],
   "id": "ee5edfa3ae3bfcbe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: tensor([[[0.4300, 0.1500, 0.8900],\n",
      "         [0.5500, 0.8200, 0.6300],\n",
      "         [0.2200, 0.1800, 0.0500],\n",
      "         [0.7600, 0.5900, 0.9200]],\n",
      "\n",
      "        [[0.4300, 0.1500, 0.8900],\n",
      "         [0.5500, 0.8200, 0.6300],\n",
      "         [0.2200, 0.1800, 0.0500],\n",
      "         [0.7600, 0.5900, 0.9200]],\n",
      "\n",
      "        [[0.4300, 0.1500, 0.8900],\n",
      "         [0.5500, 0.8200, 0.6300],\n",
      "         [0.2200, 0.1800, 0.0500],\n",
      "         [0.7600, 0.5900, 0.9200]]])\n",
      "batch.shape: torch.Size([3, 4, 3])\n",
      "context_vecs.shape: torch.Size([3, 4, 2])\n",
      "context_vecs: tensor([[[-0.0667,  0.2926],\n",
      "         [-0.0666,  0.2909],\n",
      "         [-0.0668,  0.2924],\n",
      "         [-0.0666,  0.2912]],\n",
      "\n",
      "        [[-0.0667,  0.2926],\n",
      "         [-0.0666,  0.2909],\n",
      "         [-0.0668,  0.2924],\n",
      "         [-0.0666,  0.2912]],\n",
      "\n",
      "        [[-0.0667,  0.2926],\n",
      "         [-0.0666,  0.2909],\n",
      "         [-0.0668,  0.2924],\n",
      "         [-0.0666,  0.2912]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Multi-Head Attention\n",
    "Multi-head attention use multiple causual attention module.\n",
    "\n",
    "The output will be the joint of each of head."
   ],
   "id": "7e0b08703f29193b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T09:46:52.150425Z",
     "start_time": "2025-11-01T09:46:52.130902Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([\n",
    "            CausalAttention(d_in, d_out, context_length, dropout, qkv_bias) for _ in range(num_heads)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim = -1)\n",
    "\n",
    "context_length = batch.shape[1]\n",
    "ca = MultiHeadAttentionWrapper(input_dim, output_dim, context_length, 0.0, 2)\n",
    "context_vecs = ca(batch)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)\n",
    "print(\"context_vecs:\", context_vecs)"
   ],
   "id": "462adb4fbf0fd9f8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs.shape: torch.Size([3, 4, 4])\n",
      "context_vecs: tensor([[[-0.0862, -0.0568,  0.3301,  0.0884],\n",
      "         [-0.0837, -0.0570,  0.3283,  0.0893],\n",
      "         [-0.0788, -0.0587,  0.3303,  0.0893],\n",
      "         [-0.0874, -0.0564,  0.3260,  0.0886]],\n",
      "\n",
      "        [[-0.0862, -0.0568,  0.3301,  0.0884],\n",
      "         [-0.0837, -0.0570,  0.3283,  0.0893],\n",
      "         [-0.0788, -0.0587,  0.3303,  0.0893],\n",
      "         [-0.0874, -0.0564,  0.3260,  0.0886]],\n",
      "\n",
      "        [[-0.0862, -0.0568,  0.3301,  0.0884],\n",
      "         [-0.0837, -0.0570,  0.3283,  0.0893],\n",
      "         [-0.0788, -0.0587,  0.3303,  0.0893],\n",
      "         [-0.0874, -0.0564,  0.3260,  0.0886]]], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T10:00:35.931Z",
     "start_time": "2025-11-01T10:00:35.907029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        # head_dim 是每个头的输出维度\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        # 相乘之后，keys, values, queries 都变成了 (b, num_tokens, d_out)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "        queries = self.W_query(x)\n",
    "        assert keys.shape == (b, num_tokens, self.d_out), \"keys shape must be (b, num_tokens, d_out)\"\n",
    "        assert values.shape == (b, num_tokens, self.d_out), \"values shape must be (b, num_tokens, d_out)\"\n",
    "        assert queries.shape == (b, num_tokens, self.d_out), \"queries shape must be (b, num_tokens, d_out)\"\n",
    "\n",
    "        # 把 d_out 拆分成 num_heads 个头，每个头的维度是 head_dim\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # 把维度 (b, num_tokens, num_heads, head_dim) 转换为 (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(-2, -1)\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        attn_scores.masked_fill(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim = -1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vec =(attn_weights @ values).transpose(1, 2)\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "        return context_vec\n",
    "\n",
    "context_length = batch.shape[1]\n",
    "mha = MultiHeadAttention(input_dim, output_dim, context_length, 0.0, 2)\n",
    "context_vecs = mha(batch)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)\n",
    "print(\"context_vecs:\", context_vecs)"
   ],
   "id": "18e8c1ea685686d1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs.shape: torch.Size([3, 4, 2])\n",
      "context_vecs: tensor([[[ 0.1389, -0.3722],\n",
      "         [ 0.1377, -0.3716],\n",
      "         [ 0.1377, -0.3721],\n",
      "         [ 0.1386, -0.3720]],\n",
      "\n",
      "        [[ 0.1389, -0.3722],\n",
      "         [ 0.1377, -0.3716],\n",
      "         [ 0.1377, -0.3721],\n",
      "         [ 0.1386, -0.3720]],\n",
      "\n",
      "        [[ 0.1389, -0.3722],\n",
      "         [ 0.1377, -0.3716],\n",
      "         [ 0.1377, -0.3721],\n",
      "         [ 0.1386, -0.3720]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Generative Pre-trained Transformer",
   "id": "8d289bf1cc97fa49"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"dropout\": 0.1,\n",
    "    \"qkv_bias\": True\n",
    "}\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(config[\"vocab_size\"], config[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(config[\"context_length\"], config[\"emb_dim\"])\n",
    "        self.drop_emb ="
   ],
   "id": "eb54407c4c75abb1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
