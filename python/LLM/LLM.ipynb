{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Handle Text\n",
    "\n",
    "## Text Tokenization\n",
    "\n",
    "### Download and Read the Verdict\n",
    "Download the Verdict as a text file, then read the file content."
   ],
   "id": "463d076a7403d0fb"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-28T05:43:40.374395Z",
     "start_time": "2025-10-28T05:43:39.302226Z"
    }
   },
   "source": [
    "import urllib.request\n",
    "from ftplib import ftpcp\n",
    "from msilib import type_key\n",
    "\n",
    "url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "       \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "       \"the-verdict.txt\")\n",
    "file_path = \"./the-verdict.txt\"\n",
    "urllib.request.urlretrieve(url, file_path)\n",
    "\n",
    "with open(file_path, \"r\") as file:\n",
    "    raw_text = file.read()\n",
    "print(f\"Total characters: {len(raw_text)}\")\n",
    "print(raw_text[:100])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no g\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Split the Text into Words Using Regex",
   "id": "1c4c96efd9d908e8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T05:43:40.405735Z",
     "start_time": "2025-10-28T05:43:40.390376Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(len(preprocessed))"
   ],
   "id": "6865b8430a1474d7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Convert the Words to Unique IDs\n",
   "id": "b75a48137d4d231e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T05:43:40.452070Z",
     "start_time": "2025-10-28T05:43:40.433894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "print(vocab_size)\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break"
   ],
   "id": "dabdd5fc4bdd2e03",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n",
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Simple Tokenizer\n",
    "Implement a encode function to convert a text into a sequence of token IDs.\n",
    "\n",
    "Implement a decode function to convert a sequence of token IDs back into a text."
   ],
   "id": "23f4347a09f1b8e3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T05:43:40.498222Z",
     "start_time": "2025-10-28T05:43:40.486181Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.inverse_vocab = {integer:token for token, integer in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        return [self.vocab[token] for token in preprocessed]\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = ' '.join([self.inverse_vocab[id] for id in ids])\n",
    "        text = re.sub(r'\\s+([,.:;?_!\"()\\'])', r'\\1', text)\n",
    "        return text\n",
    "\n",
    "tokenizerV1 = SimpleTokenizerV1(vocab)\n",
    "text = \"\"\"It's the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\"\"\"\"\"\n",
    "ids = tokenizerV1.encode(text)\n",
    "print(ids)\n",
    "decoded_text = tokenizerV1.decode(ids)\n",
    "print(decoded_text)"
   ],
   "id": "1cae35ccaf02aa2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n",
      "It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Tokenizer V2\n",
    "We want to handle the unknown words in the text.\n",
    "We will add a special token `<|unk|>` to represent the unknown words.\n",
    "\n",
    "And add a token `<|endoftext|>` to represent the end of the text, thus we can handle multiple texts that from different sources."
   ],
   "id": "38f678f921606de"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T05:45:20.732515Z",
     "start_time": "2025-10-28T05:45:20.723378Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# add the special tokens to the vocab\n",
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
    "\n",
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.inverse_vocab = {integer:token for token, integer in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        return [self.vocab.get(token, self.vocab[\"<|unk|>\"]) for token in preprocessed]\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = ' '.join([self.inverse_vocab[id] for id in ids])\n",
    "        text = re.sub(r'\\s+([,.:;?_!\"()\\'])', r'\\1', text)\n",
    "        return text\n",
    "\n",
    "tokenizerV2 = SimpleTokenizerV2(vocab)\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "ids = tokenizerV2.encode(text)\n",
    "print(ids)\n",
    "decoded_text = tokenizerV2.decode(ids)\n",
    "print(decoded_text)\n"
   ],
   "id": "54496b82e79e7e22",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]\n",
      "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### BPE Tokenizer\n",
    "BPE could handle the unknown words in the text by split it into smaller tokens.\n",
    "\n",
    "BPE merge the most frequent pairs of tokens into a new token to make the vocabulary."
   ],
   "id": "55d467d2cf68a428"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T06:06:58.033628Z",
     "start_time": "2025-10-28T06:06:58.016899Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "text = \"Aiwerkn oker\"\n",
    "ids = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(ids)\n",
    "for id in ids:\n",
    "    print(id, tokenizer.decode([id]))\n",
    "print(tokenizer.decode(ids))"
   ],
   "id": "cebd7becb1ac5db9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32, 14246, 9587, 77, 267, 6122]\n",
      "32 A\n",
      "14246 iw\n",
      "9587 erk\n",
      "77 n\n",
      "267  o\n",
      "6122 ker\n",
      "Aiwerkn oker\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Use Sliding Window to Create Input-Output Pairs\n",
    "### Read and Encode the Verdict"
   ],
   "id": "75b4d72d4d495fd7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T06:14:34.889124Z",
     "start_time": "2025-10-28T06:14:34.880125Z"
    }
   },
   "cell_type": "code",
   "source": [
    "file_path = \"./the-verdict.txt\"\n",
    "with open(file_path, \"r\") as file:\n",
    "    raw_text = file.read()\n",
    "enc_text = tokenizer.encode(raw_text, allowed_special={\"<|endoftext|>\"})\n",
    "print(len(enc_text))"
   ],
   "id": "c95e6e986b98f227",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Create Input-Output Pairs\n",
    "The input will be a sequence of tokens.\n",
    "\n",
    "The output will be the next token in the sequence."
   ],
   "id": "63b6c1a39352d381"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T06:19:14.706589Z",
     "start_time": "2025-10-28T06:19:14.695580Z"
    }
   },
   "cell_type": "code",
   "source": [
    "enc_sample = enc_text[:10]\n",
    "context_size = 4\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size + 1]\n",
    "\n",
    "for i in range(1, context_size + 1):\n",
    "    context = enc_sample[:i]\n",
    "    target = enc_sample[i]\n",
    "    print(context, \"->\", target)\n",
    "    print(tokenizer.decode(context), \"->\", tokenizer.decode([target]))"
   ],
   "id": "316d095a9cb58efa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40] -> 367\n",
      "I ->  H\n",
      "[40, 367] -> 2885\n",
      "I H -> AD\n",
      "[40, 367, 2885] -> 1464\n",
      "I HAD ->  always\n",
      "[40, 367, 2885, 1464] -> 1807\n",
      "I HAD always ->  thought\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### DataSet and DataLoader\n",
    "We use the sliding window to create the input-output pairs.\n",
    "\n",
    "x is the input sequence of tokens, specifically `text[sample_start: sample_start + context_size]`.\n",
    "\n",
    "y is the target sequence of tokens, specifically `text[sample_start + 1: sample_start + context_size + 1]`.\n",
    "\n",
    "We could generate the input-output pairs from x and y as forementioned.\n",
    "\n",
    "DataSet is to store the tokenized text. DataLoader is to load the data in batches."
   ],
   "id": "2db3f57302f23929"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T09:21:03.496621Z",
     "start_time": "2025-10-28T09:21:03.432963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDataSetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size = 4, max_length = 256, stride = 128, shuffle = True, drop_last = True, num_workers = 0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDataSetV1(txt, tokenizer, max_length, stride)\n",
    "    return DataLoader(dataset, batch_size = batch_size, shuffle = shuffle, drop_last = drop_last, num_workers = num_workers)\n",
    "\n",
    "file_path = \"./the-verdict.txt\"\n",
    "with open(file_path, \"r\") as file:\n",
    "    raw_text = file.read()\n",
    "\n",
    "dataLoader = create_dataloader_v1(raw_text, batch_size = 8, max_length = 4, stride = 4, shuffle = False, drop_last = True, num_workers = 0)\n",
    "data_iter = iter(dataLoader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)\n"
   ],
   "id": "5870736813c53745",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]]), tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])]\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Embedding Layer\n",
    "\n",
    "### A Simple Embedding Layer\n",
    "As a simple example, we create an embedding layer.\n",
    "\n",
    "The embedding layer has two parameters:\n",
    "1. The vocabulary size, which is the number of unique tokens in the dataset.\n",
    "2. The embedding dimension, which is the size of the vector representation for each token.\n",
    "\n",
    "The embedding layer receives a tensor of token indices and returns a tensor of token embeddings.\n",
    "For example, the output dim is 5 and the num of tokens is 32, then it will return a tensor of shape (5, 32)."
   ],
   "id": "7e7c000ba4ab83f6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T09:30:09.365859Z",
     "start_time": "2025-10-28T09:30:09.267570Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vocab_size = tokenizer.n_vocab\n",
    "embedding_dim = 32\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "print(embedding_layer(torch.tensor([3])))"
   ],
   "id": "19e756358679a889",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7585, -0.7283, -1.0025, -0.0571,  1.0934, -0.0509, -0.6770, -0.4610,\n",
      "          0.8019,  1.9966, -0.6660,  1.0782, -0.0667,  0.3993,  1.7781, -1.3685,\n",
      "          1.1044,  1.5756, -0.0521, -0.0376,  0.9942,  0.4432, -0.3715,  1.8533,\n",
      "         -0.0155, -0.5434, -0.5900,  1.9482, -1.4059,  0.5269,  1.5739,  0.1289]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "execution_count": 46
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
